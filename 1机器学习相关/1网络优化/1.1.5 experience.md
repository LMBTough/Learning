1. 先别急着写代码，观察**数据规律**，因为神经网络学习到的是数据的规律，可能会发现图像和标签之中的错误。
2. 设置端到端的训练评估框架，选择有一个简单不至于搞砸的模型，可视化损失，
3. **固定随机种子**，另外尽可能简单不要扩增数据。绘制测试集损失时，对着呢哥哥测试集进行评估，不要只绘制批次测试损失图像。
4. 在初始阶段验证下损失函数，验证函数是否从正确的损失值开始。例如，如果正确初始化最后一层，则应在softmax初始化时测量-log(1/n_classes)。
5. 初始化，正确初始化最后一层的权重。如果回归一些平均值为50的值，则将最终偏差初始化为50。如果有一个比例为1:10的不平衡数据集，请设置对数的偏差，使网络预测概率在初始化时为0.1。正确设置这些可以加速模型的收敛。
6. 人类基线 监控除人为可解释和可检查的损失之外的指标。尽可能评估人的准确性并与之进行比较。或者对测试数据进行两次注释，并且对于每个示例，将一个注释视为预测，将第二个注释视为事实。
7. 设置一个独立于输入的基线，最简单的方法是**将所有输入设置为零**，看看模型是否学会从输入中提取任何信息。
8. 过拟合一个batch， 增加了模型的容量并验证我们可以达到的最低损失。
9. 在训练模型前进行数据可视化，将原始张量的数据和标签可视化，可以节省了调试次数，并揭示了数据预处理和数据扩增中的问题。
10. 可视化预测动态， 在训练过程中对固定测试批次上的模型预测进行可视化。
11. Adam方法是安全的
12. 一次只复杂化一个，如果多个信号输入分类器，建议逐个输入，然后增加复杂性，确保预期的性能逐步提升，而不要一股脑儿全放进去。比如，尝试先插入较小的图像，然后再将它们放大。
13. **不要相信学习率衰减默认值**，如果不小心，代码可能会过早地将学习率减少到零，导致模型无法收敛。我们完全禁用学习率衰减避免这种状况的发生。
14.** 更多数据数据扩增，预训练(如果数据量大),坚持监督学习，输入低维度一点。**
15. **模型小一点，减少批尺寸，对批量归一化**（Batch Normalization）来讲，小批量效果更好。
16. Dropout，给卷积网络用dropout2d。不过使用需谨慎，因为这种操作似乎跟批量归一化不太合得来。
17. 权重衰减，增加权重衰减 (Weight Decay) 的惩罚力度。
18. 早停法，不用一直一直训练，可以观察验证集的损失，在快要过拟合的时候，及时喊停。
19. 大模型容易过拟合，几乎是必然，可以用早停。
20. 最后的最后，如果想要更加确信，自己训练出的网络，是个不错的分类器，就把第一层的权重可视化一下，看看边缘 (Edges) 美不美。**Feature Map**
21. 随机网格搜索在同时调整多个超参数的情况下，网格搜索听起来是很诱人，可以把各种设定都包含进来。但是要记住，**随机搜索**才是最好的。直觉上说，这是因为网络通常对其中一些参数比较敏感，对其他参数不那么敏感。如果参数a是有用的，参数b起不了什么作用，就应该对a取样更彻底一些，不要只在几个固定点上多次取样。
22. **模型合体**把几个模型结合在一起，至少可以保证提升2%的准确度，不管是什么任务。如果，你买不起太多的算力，就用蒸馏 (Distill) 把模型们集合成一个神经网络。
